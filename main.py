"""
FastAPI åº”ç”¨å…¥å£
æä¾› OpenAI å…¼å®¹çš„ HTTP æ¥å£
"""
import uuid
import json
import time
from typing import AsyncGenerator

from fastapi import FastAPI, HTTPException, Request, status
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

from models import (
    ChatRequest,
    ChatCompletionChunk,
    Choice,
    Delta,
    ErrorResponse,
    ErrorDetail,
)
from iflow_service import iflow_service, IFlowConnectionError

# å¸¸é‡å®šä¹‰
EXPECTED_TOKEN = "111222333444555666"
EXPECTED_MODEL = "iflow"


# ==================== FastAPI åº”ç”¨åˆå§‹åŒ– ====================

app = FastAPI(
    title="iFlow OpenAI Compatible API",
    description="å°† iFlow AI æ¨¡å‹åŒ…è£…ä¸º OpenAI å…¼å®¹æ¥å£",
    version="1.0.0",
)

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== åº”ç”¨ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ ====================

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶ - å¯åŠ¨ iFlow è¿›ç¨‹å¹¶åˆå§‹åŒ–é•¿æœŸè¿æ¥"""
    import subprocess
    import time
    import socket

    print("=" * 60)
    print("æ­£åœ¨å¯åŠ¨ iFlow OpenAI å…¼å®¹ API æœåŠ¡...")
    print("=" * 60)

    # æ£€æŸ¥ iFlow CLI æ˜¯å¦å·²å®‰è£…
    try:
        result = subprocess.run(["iflow", "--version"], capture_output=True, text=True)
        if result.returncode != 0:
            print("âš ï¸  iFlow CLI æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… iFlow CLI")
            print("å®‰è£…å‘½ä»¤: npm install -g iflow-cli")
        else:
            print(f"âœ… iFlow CLI å·²å®‰è£…: {result.stdout.strip()}")
    except FileNotFoundError:
        print("âš ï¸  iFlow CLI æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… iFlow CLI")
        print("å®‰è£…å‘½ä»¤: npm install -g iflow-cli")

    # å°è¯•å¯åŠ¨ iFlow è¿›ç¨‹
    try:
        # æ£€æŸ¥ç«¯å£æ˜¯å¦å·²è¢«å ç”¨
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('127.0.0.1', 8090))
        sock.close()

        if result != 0:
            print("ğŸš€ å¯åŠ¨ iFlow è¿›ç¨‹...")
            # å¯åŠ¨ iFlow è¿›ç¨‹ï¼Œå¯ç”¨ ACP æœåŠ¡
            process = subprocess.Popen(
                ["iflow", "--experimental-acp", "--port", "8090"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                start_new_session=True
            )
            print("âœ… iFlow è¿›ç¨‹å·²å¯åŠ¨ï¼ˆåå°è¿è¡Œï¼‰")
            print("â³ ç­‰å¾… iFlow è¿›ç¨‹å°±ç»ª...")
            # ç­‰å¾… 5 ç§’ï¼Œç¡®ä¿è¿›ç¨‹å°±ç»ª
            time.sleep(5)
        else:
            print("âš ï¸  ç«¯å£ 8090 å·²è¢«å ç”¨ï¼Œä½¿ç”¨å·²æœ‰çš„ iFlow è¿›ç¨‹")
    except Exception as e:
        print(f"âš ï¸  å¯åŠ¨ iFlow è¿›ç¨‹å¤±è´¥: {e}")
        print("è¯·æ‰‹åŠ¨å¯åŠ¨: iflow --experimental-acp --port 8090")

    # æœåŠ¡å¯åŠ¨å®Œæˆ
    print("=" * 60)
    print("âœ… æœåŠ¡å¯åŠ¨å®Œæˆï¼")
    print("æ¥å£åœ°å€: http://127.0.0.1:11666/v1")
    print("å¥åº·æ£€æŸ¥: http://127.0.0.1:11666/health")
    print("=" * 60)


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶"""
    print("=" * 60)
    print("æ­£åœ¨å…³é—­æœåŠ¡...")
    print("=" * 60)

    # å…³é—­ iFlow è¿æ¥
    await iflow_service.close()

    print("=" * 60)
    print("âœ… æœåŠ¡å·²å…³é—­")
    print("=" * 60)


# ==================== è¾…åŠ©å‡½æ•° ====================

def create_error_response(
    message: str,
    error_type: str = "invalid_request_error",
    status_code: int = status.HTTP_400_BAD_REQUEST,
) -> tuple[ErrorResponse, int]:
    """
    åˆ›å»ºé”™è¯¯å“åº”

    Args:
        message: é”™è¯¯æ¶ˆæ¯
        error_type: é”™è¯¯ç±»å‹
        status_code: HTTP çŠ¶æ€ç 

    Returns:
        (ErrorResponse, status_code)
    """
    error = ErrorResponse(
        error=ErrorDetail(message=message, type=error_type)
    )
    return error, status_code


async def validate_request(request: Request) -> None:
    """
    éªŒè¯è¯·æ±‚

    Args:
        request: FastAPI è¯·æ±‚å¯¹è±¡

    Raises:
        HTTPException: éªŒè¯å¤±è´¥
    """
    # éªŒè¯ Authorization header
    auth_header = request.headers.get("Authorization")

    if not auth_header:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=create_error_response(
                "Missing Authorization header",
                error_type="authentication_error"
            )[0].model_dump()
        )

    # éªŒè¯ token æ ¼å¼
    if not auth_header.startswith("Bearer "):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=create_error_response(
                "Invalid Authorization header format",
                error_type="authentication_error"
            )[0].model_dump()
        )

    # éªŒè¯ token å€¼
    token = auth_header.replace("Bearer ", "").strip()  # å»é™¤æ‰€æœ‰ "Bearer " å¹¶å»é™¤å‰åç©ºæ ¼

    if token != EXPECTED_TOKEN:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=create_error_response(
                "Invalid authentication token",
                error_type="authentication_error"
            )[0].model_dump()
        )


async def stream_generator(
    messages: list, request_id: str
) -> AsyncGenerator[str, None]:
    """
    æµå¼å“åº”ç”Ÿæˆå™¨

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        request_id: è¯·æ±‚ ID

    Yields:
        str: SSE æ ¼å¼çš„æ•°æ®å—
    """
    created_timestamp = int(time.time())

    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æ¶ˆæ¯
        has_user_message = False
        for msg in messages:
            if msg.role == "user":
                has_user_message = True
                break

        if not has_user_message:
            raise ValueError("No user message found")

        # è°ƒç”¨ iFlow SDK è·å–æµå¼å“åº”ï¼ˆä¼ é€’å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼‰
        async for text_chunk in iflow_service.stream_chat(messages):
            if text_chunk:
                # åˆ›å»º OpenAI æ ¼å¼çš„ chunk
                chunk = ChatCompletionChunk(
                    id=request_id,
                    created=created_timestamp,
                    model=EXPECTED_MODEL,
                    choices=[
                        Choice(
                            index=0,
                            delta=Delta(content=text_chunk),
                            finish_reason=None
                        )
                    ]
                )

                # è¿”å› SSE æ ¼å¼
                yield f"data: {chunk.model_dump_json()}\n\n"

        # å‘é€ç»“æŸä¿¡å·
        final_chunk = ChatCompletionChunk(
            id=request_id,
            created=created_timestamp,
            model=EXPECTED_MODEL,
            choices=[
                Choice(
                    index=0,
                    delta=Delta(content=None),
                    finish_reason="stop"
                )
            ]
        )
        yield f"data: {final_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"

    except IFlowConnectionError as e:
        # iFlow è¿æ¥é”™è¯¯
        error_chunk = ChatCompletionChunk(
            id=request_id,
            created=created_timestamp,
            model=EXPECTED_MODEL,
            choices=[]
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"

    except Exception as e:
        # å…¶ä»–é”™è¯¯
        error_chunk = ChatCompletionChunk(
            id=request_id,
            created=created_timestamp,
            model=EXPECTED_MODEL,
            choices=[]
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


# ==================== API è·¯ç”± ====================

@app.get(
    "/v1/models",
    response_model=None,
    responses={
        200: {"description": "æ¨¡å‹åˆ—è¡¨"},
        401: {"description": "è®¤è¯å¤±è´¥", "model": ErrorResponse},
    },
)
async def list_models(request: Request):
    """
    åˆ—å‡ºå¯ç”¨æ¨¡å‹

    OpenAI å…¼å®¹æ¥å£ - è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    await validate_request(request)

    return {
        "object": "list",
        "data": [
            {
                "id": "iflow",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "iflow"
            }
        ]
    }


@app.get(
    "/v1/models/{model_id}",
    response_model=None,
    responses={
        200: {"description": "æ¨¡å‹ä¿¡æ¯"},
        401: {"description": "è®¤è¯å¤±è´¥", "model": ErrorResponse},
        404: {"description": "æ¨¡å‹ä¸å­˜åœ¨", "model": ErrorResponse},
    },
)
async def retrieve_model(request: Request, model_id: str):
    """
    è·å–æ¨¡å‹ä¿¡æ¯

    OpenAI å…¼å®¹æ¥å£ - è¿”å›æŒ‡å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
    """
    await validate_request(request)

    if model_id != "iflow":
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=create_error_response(
                f"Model '{model_id}' not found",
                error_type="invalid_request_error"
            )[0].model_dump()
        )

    return {
        "id": "iflow",
        "object": "model",
        "created": int(time.time()),
        "owned_by": "iflow"
    }


@app.post(
    "/v1/chat/completions",
    response_model=None,
    responses={
        200: {"description": "æµå¼å“åº”"},
        400: {"description": "è¯·æ±‚é”™è¯¯", "model": ErrorResponse},
        401: {"description": "è®¤è¯å¤±è´¥", "model": ErrorResponse},
    },
)
async def chat_completions(request: Request, body: ChatRequest):
    """
    OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£

    æ”¯æŒï¼š
    - æµå¼å“åº”ï¼ˆstream=trueï¼‰
    - OpenAI æ ¼å¼çš„è¯·æ±‚å’Œå“åº”
    - æ ‡å‡† SSE åè®®

    é™åˆ¶ï¼š
    - ä»…æ”¯æŒ model="iflow"
    - ä»…æ”¯æŒ stream=true
    - éœ€è¦æœ‰æ•ˆçš„ Authorization header
    """
    # éªŒè¯è¯·æ±‚
    await validate_request(request)

    # ç”Ÿæˆè¯·æ±‚ ID
    request_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"

    # è¿”å›æµå¼å“åº”
    return StreamingResponse(
        stream_generator(body.messages, request_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ nginx ç¼“å†²
        }
    )


# ==================== å¥åº·æ£€æŸ¥ ====================

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "ok", "service": "iflow-openai-compatible-api"}


# ==================== å¯åŠ¨å…¥å£ ====================

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="127.0.0.1",
        port=11666,
        reload=False,
        log_level="info"
    )